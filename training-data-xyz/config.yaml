# NequIP training configuration for Ag-O-Cl system

# This config file uses OmegaConf and Hydra for structured configuration.
# Key references:
# - https://hydra.cc/docs/intro/
# - https://omegaconf.readthedocs.io/

# =========
#    RUN
# =========
# List of tasks to run in sequence. Typically [train, test] or [train, val, test]
run: [train, test]

# ==========
#   GLOBAL VARIABLES
# ==========
cutoff_radius: 5.0                      # cutoff distance for neighbor list
chemical_symbols: [Ag, O, Cl]           # atomic species in dataset
model_type_names: ${chemical_symbols}   # type names used internally by the model

# ============
#    DATA
# ============
data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456

  # Specify dataset files (all .xyz files in working directory)
  split_dataset:
    file_path: ./alltraj.xyz
    train: 0.8
    val: 0.1
    test: 0.1

  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}            # build neighbor list with cutoff
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}  # map chemical symbols to atom types

  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 5
    num_workers: 5
    shuffle: true

  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 10
    num_workers: ${data.train_dataloader.num_workers}

  test_dataloader: ${data.val_dataloader}

  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    dataloader_kwargs:
      batch_size: 10
    type_names: ${model_type_names}

# ============
#   TRAINER
# ============
trainer:
  _target_: lightning.Trainer
  accelerator: auto
  enable_checkpointing: true
  max_epochs: 1000
  max_time: 03:00:00:00
  check_val_every_n_epoch: 1
  log_every_n_steps: 1

  logger:
    _target_: lightning.pytorch.loggers.csv_logs.CSVLogger
    name: tutorial_log
    save_dir: ./results
    flush_logs_every_n_steps: 100

  callbacks:
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: "val0_epoch/weighted_sum"
      min_delta: 1e-3
      patience: 20

    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: "val0_epoch/weighted_sum"
      dirpath: ./results
      filename: best
      save_last: true

    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

# ======================
#   TRAINING MODULE
# ======================
training_module:
  _target_: nequip.train.EMALightningModule

  ema_decay: 0.999                      # exponential moving average decay for weights

  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 1.0
      forces: 1.0

  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      total_energy_mae: 1.0
      forces_mae: 1.0

  train_metrics: ${training_module.val_metrics}
  test_metrics: ${training_module.val_metrics}

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.03

  lr_scheduler:
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.6
      patience: 5
      threshold: 0.2
      min_lr: 1e-6
    monitor: "val0_epoch/weighted_sum"
    interval: epoch
    frequency: 1

  model:
    _target_: nequip.model.NequIPGNNModel
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    num_bessels: 8
    bessel_trainable: false
    polynomial_cutoff_p: 6

    num_layers: 3
    l_max: 1
    parity: true
    num_features: 32

    radial_mlp_depth: 2
    radial_mlp_width: 64

    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}
    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal
      chemical_species: ${chemical_symbols}

# ===================
#   GLOBAL OPTIONS
# ===================
global_options:
  allow_tf32: false
